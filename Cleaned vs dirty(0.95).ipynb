{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T15:27:33.091582Z",
     "iopub.status.busy": "2022-03-04T15:27:33.091266Z",
     "iopub.status.idle": "2022-03-04T15:27:34.578586Z",
     "shell.execute_reply": "2022-03-04T15:27:34.577700Z",
     "shell.execute_reply.started": "2022-03-04T15:27:33.091546Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "import copy\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Распаковка исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#Выведем файлы, которые  расположены в корневой папке проекта - input\n",
    "print(os.listdir('../input'),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Распакуем данные из архива в директорию working\n",
    "with zipfile.ZipFile('../input/platesv2/plates.zip', 'r') as zip_obj:\n",
    "        zip_obj.extractall('/kaggle/working/') \n",
    "print(os.listdir('/kaggle/working/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/kaggle/working/plates/' \n",
    "print(os.listdir(data_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определяем функции для обработки изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Удаляем фон \n",
    "class Remove_background_and_crop:\n",
    "    \n",
    "    def __init__(self,img):\n",
    "        self.x00 = 0\n",
    "        self.x00 = 0\n",
    "        self.r00 = 0\n",
    "        self.img = img\n",
    "        self.mask = img\n",
    "    \n",
    "    \n",
    "    #Обрезаем изображения для увеличения train датасета\n",
    "    def crop (self): \n",
    "            c_r_crop = (1.42*self.r00/2)\n",
    "            self.img = self.img[int(self.y00)-int(c_r_crop):int(self.y00)+int(c_r_crop),int(self.x00)-int(c_r_crop):int(self.x00)+int(c_r_crop)]\n",
    "            self.img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n",
    "            crop = self.img\n",
    "            cv2.imwrite(image_folder,crop)\n",
    "            h,w = self.img.shape[:2] \n",
    "            c = min(h,w)     \n",
    "            for i in range (5,int(c/3),5): \n",
    "                    crop_img = self.img[i:h-i,i:w-i]    \n",
    "                    cv2.imwrite(image_folder[:-4] +  '_Crop_' + str(i) + '.jpg',crop_img)\n",
    "            image1 = self.img[0:int(h//2),0:int(w//2)]\n",
    "            #Делим изображение на 4 части\n",
    "            cv2.imwrite(image_folder[:-4] +  'image1' + '.jpg',image1)\n",
    "            image2 = self.img[int(h//2):h,int(w//2):w]\n",
    "            cv2.imwrite(image_folder[:-4] +  'image2' + '.jpg',image2)\n",
    "            image3 = self.img[int(h//2):h,0:int(w//2)]\n",
    "            cv2.imwrite(image_folder[:-4] +  'image3' + '.jpg',image3)\n",
    "            image4 = self.img[0:int(h//2),int(w//2):w]\n",
    "            cv2.imwrite(image_folder[:-4] +  'image4' + '.jpg',image4)\n",
    "            \n",
    "#Для test датасета оставляем изображение вырезанное по вписанному квадрату       \n",
    "    def crop_test (self):\n",
    "        c_r_crop = (1.42*self.r00/2)\n",
    "        self.img = self.img[int(self.y00)-int(c_r_crop):int(self.y00)+int(c_r_crop),int(self.x00)-int(c_r_crop):int(self.x00)+int(c_r_crop)]\n",
    "        self.img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(image_folder,self.img)       \n",
    "     \n",
    " #Ищем круги в пределах координат\n",
    "    def find_circle(self):\n",
    "        output = self.img.copy()    \n",
    "        img = cv2.convertScaleAbs(self.img, alpha=1.2, beta=0.0)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 10,param1=10,param2=5,minRadius=40,maxRadius=250)\n",
    "        \n",
    "        if circles is not None: \n",
    "            circles = np.round(circles[0, :]).astype(\"int\")\n",
    "                                         \n",
    "            for x, y, r in circles:\n",
    "                if ((self.x00-15)<x<(self.x00+15)) and ((self.y00-15)<y<(self.y00+15)):\n",
    "                    if r > self.r00: \n",
    "                        self.x00 = x\n",
    "                        self.y00 = y\n",
    "                        self.r00 = r\n",
    "                        \n",
    "                    \n",
    "            if self.r00==0: \n",
    "                ret,thresh = cv2.threshold(self.mask,235,255,0)\n",
    "                contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)   \n",
    "                c = max(contours, key=cv2.contourArea)\n",
    "                (self.x00, self.y00), self.r00 = cv2.minEnclosingCircle(c)\n",
    "                        \n",
    "            \n",
    "    #Находим координаты центра фигуры\n",
    "    def findCoordinates(self):  \n",
    "        ret,thresh = cv2.threshold(self.mask,235,255,0)        \n",
    "        M = cv2.moments(thresh)\n",
    "        self.x00 = int(M[\"m10\"] / M[\"m00\"])\n",
    "        self.y00 = int(M[\"m01\"] / M[\"m00\"])\n",
    "        \n",
    "\n",
    "                \n",
    "    def remove_background(self):  \n",
    "        mainRectSize = .08\n",
    "        fgSize = .01\n",
    "        img = self.img\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        new_h, new_w = img.shape[:2]\n",
    "        mask = np.zeros(img.shape[:2], np.uint8)\n",
    "        bg_w = round(new_w * mainRectSize)\n",
    "        bg_h = round(new_h * mainRectSize)\n",
    "        bg_rect = (bg_w, bg_h, new_w - bg_w, new_h - bg_h)\n",
    "        fg_w = round(new_w * (1 - fgSize) / 2)\n",
    "        fg_h = round(new_h * (1 - fgSize) / 2)\n",
    "        fg_rect = (fg_w, fg_h, new_w - fg_w, new_h - fg_h)\n",
    "        cv2.rectangle(mask, fg_rect[:2], fg_rect[2:4], color=cv2.GC_FGD, thickness=-1)\n",
    "        bgdModel1 = np.zeros((1, 65), np.float64)\n",
    "        fgdModel1 = np.zeros((1, 65), np.float64)\n",
    "        cv2.grabCut(img, mask, bg_rect, bgdModel1, fgdModel1, 3, cv2.GC_INIT_WITH_RECT)\n",
    "        cv2.rectangle(mask, bg_rect[:2], bg_rect[2:4], color=cv2.GC_PR_BGD, thickness=bg_w * 3)\n",
    "        cv2.grabCut(img, mask, bg_rect, bgdModel1, fgdModel1, 10, cv2.GC_INIT_WITH_MASK)   \n",
    "        mask_result = np.where((mask == 1) + (mask == 3), 255, 0).astype('uint8')\n",
    "        masked = cv2.bitwise_and(img, img, mask=mask_result)\n",
    "        masked[mask_result < 2] = [255, 255, 255] \n",
    "        self.img = masked\n",
    "        self.mask = mask_result\n",
    "    \n",
    "for image_index in range (20):\n",
    "    print (\"Complete dirty: \",\"{0:04}\".format(image_index),\"/0019\", end=\"\\r\")\n",
    "    image_folder = '/kaggle/working/plates/train/dirty/{0:04}.jpg'.format(image_index) \n",
    "    img = cv2.imread(image_folder)\n",
    "    out_img  = Remove_background_and_crop(img)\n",
    "    out_img.remove_background()\n",
    "    out_img.findCoordinates()\n",
    "    out_img.find_circle()\n",
    "    out_img.crop()    \n",
    "print (\"\\n\\r\", end=\"\")    \n",
    "    \n",
    "for image_index in range (20):\n",
    "    print (\"Complete cleaned: \",\"{0:04}\".format(image_index),\"/0019\", end=\"\\r\")\n",
    "    image_folder = '/kaggle/working/plates/train/cleaned/{0:04}.jpg'.format(image_index) \n",
    "    img = cv2.imread(image_folder)\n",
    "    out_img  = Remove_background_and_crop(img)\n",
    "    out_img.remove_background()\n",
    "    out_img.findCoordinates()\n",
    "    out_img.find_circle()\n",
    "    out_img.crop()\n",
    "print (\"\\n\\r\", end=\"\")\n",
    "\n",
    "for image_index in range (744):\n",
    "    print (\"Complete test: \",\"{0:04}\".format(image_index),\"/0743\", end=\"\\r\")\n",
    "    image_folder = '/kaggle/working/plates/test/{0:04}.jpg'.format(image_index) \n",
    "    img = cv2.imread(image_folder)\n",
    "    out_img  = Remove_background_and_crop(img)\n",
    "    out_img.remove_background()\n",
    "    out_img.findCoordinates()\n",
    "    out_img.find_circle()\n",
    "    out_img.crop_test()\n",
    "print (\"\\n\\r\", end=\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем папки train и val (из папки train берем каждую шестую на валидацию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train' # на этих данных будем обучать модель\n",
    "val_dir = 'val' #на этих данных будем смотреть какую accuracy показывает наша модель \n",
    "class_names = ['cleaned', 'dirty']\n",
    " \n",
    "for dir_name in [train_dir, val_dir]:\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n",
    " \n",
    "for class_name in class_names:\n",
    "    source_dir = os.path.join(data_root, 'train', class_name)\n",
    "    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n",
    "        if i % 6 != 0:\n",
    "            dest_dir = os.path.join(train_dir, class_name) \n",
    "        else:\n",
    "            dest_dir = os.path.join(val_dir, class_name)\n",
    "        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аугментация данных т.е. увеличение выборки данных для обучения через модификацию существующих данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomPerspective(distortion_scale=0.09, p=0.75, interpolation=3, fill=255),\n",
    "    transforms.Resize((224, 224)),    \n",
    "    transforms.ColorJitter(hue=(-0.5,0.5)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.8, interpolation=3, fill=255),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(hue=(-0.5,0.5)),\n",
    "    transforms.RandomHorizontalFlip(),     \n",
    "    transforms.RandomVerticalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])   \n",
    "\n",
    "dataset_transforms = {\n",
    "                      'orig': transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "\n",
    "                      '140': transforms.Compose([\n",
    "    transforms.CenterCrop(140),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                     '135': transforms.Compose([\n",
    "    transforms.CenterCrop(135),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]), \n",
    "                      '130': transforms.Compose([\n",
    "    transforms.CenterCrop(130),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                      '125': transforms.Compose([\n",
    "    transforms.CenterCrop(125),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                      '120': transforms.Compose([\n",
    "    transforms.CenterCrop(120),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                      '115': transforms.Compose([\n",
    "    transforms.CenterCrop(115),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                      '110': transforms.Compose([\n",
    "    transforms.CenterCrop(110),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                      '105': transforms.Compose([\n",
    "    transforms.CenterCrop(105),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                      '100': transforms.Compose([\n",
    "    transforms.CenterCrop(100),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                     '95': transforms.Compose([\n",
    "    transforms.CenterCrop(95),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                       '90': transforms.Compose([\n",
    "    transforms.CenterCrop(90),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                       '85': transforms.Compose([\n",
    "    transforms.CenterCrop(85),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                       '80': transforms.Compose([\n",
    "    transforms.CenterCrop(80),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "                      '75': transforms.Compose([\n",
    "    transforms.CenterCrop(75),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),                                         \n",
    "                       '70': transforms.Compose([\n",
    "    transforms.CenterCrop(70),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),                                                           \n",
    "                     }\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#передадим наши трансформации в ImageFolder\n",
    "train_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\n",
    "val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Далее формируем датасеты с размером батча, равным 16-ти\n",
    "batch_size = 16 # Количество изображений в батче\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверим кратность датасетов размеру батча\n",
    "len(train_dataloader), len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция визуализации  изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_input(input_tensor, title=''):\n",
    "    image = input_tensor.permute(1, 2, 0).numpy() #обратная операция к ToTensor - .permute(1, 2, 0) Channels,H,W -> H,W,Channels потом превращаем в numpy array .numpy()\n",
    "    image = std * image + mean #Делаем обратную трансформацию нормировки\n",
    "    plt.imshow(image.clip(0, 1))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    plt.pause(0.1)\n",
    " \n",
    "X_batch, y_batch = next(iter(train_dataloader)) \n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "for x_item, y_item in zip(X_batch, y_batch):\n",
    "    show_input(x_item, title=class_names[y_item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss, optimizer, scheduler, num_epochs):\n",
    " \n",
    "    loss_hist = {'train':[], 'val':[]}\n",
    "    acc_hist = {'train':[], 'val':[]}\n",
    " \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}:\".format(epoch, num_epochs - 1), end=\"\")\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train': #Если фаза == Тренировка  \n",
    "                dataloader = train_dataloader #берем train_dataLoader\n",
    "                scheduler.step() #Делаем 1 шаг (произошла одна эпоха)\n",
    "                model.train()  # Модель в training mode - обучение (Фиксируем модель, иначе у нас могут изменяться параметры\n",
    "                #слоя батч-нормализации и изменится нейроннная сеть с течением времени)\n",
    "            else: #Если фаза == Валидация \n",
    "                dataloader = val_dataloader #берем val_dataLoader \n",
    "                model.eval()   # Модель в evaluate mode - валидация (Фиксируем модель, иначе у нас могут изменяться параметры\n",
    "                #слоя батч-нормализации и изменится нейроннная сеть с течением времени)\n",
    " \n",
    "            running_loss = 0. \n",
    "            running_acc = 0.\n",
    " \n",
    "            # Итерируемся по dataloader\n",
    "            for inputs, labels in tqdm(dataloader):\n",
    "                inputs = inputs.to(device) # Тензор с изображениями переводим на GPU \n",
    "                labels = labels.to(device) # Тензор с лейблами переводим на GPU \n",
    " \n",
    "                optimizer.zero_grad() # Обнуляем градиент,чтобы он не накапливался \n",
    " \n",
    "                with torch.set_grad_enabled(phase == 'train'): #Если фаза train то активируем все градиенты\n",
    "              #(те которые не заморожены) (очистить историю loss)\n",
    "                    preds = model(inputs) # Считаем предикты, input передаем в модель\n",
    "                    loss_value = loss(preds, labels) #Посчитали  Loss    \n",
    "                    preds_class = preds.argmax(dim=1) # Получаем класс,берем .argmax(dim=1) нейрон с максимальной активацией\n",
    "                \n",
    "                    if phase == 'train':\n",
    "                        loss_value.backward() # Считаем градиент \n",
    "                        optimizer.step() # Считаем шаг градиентного спуска\n",
    " \n",
    "                # Статистика\n",
    "                running_loss += loss_value.item() #считаем Loss\n",
    "                running_acc += (preds_class == labels.data).float().mean().data.cpu().numpy()  #считаем accuracy\n",
    " \n",
    "            epoch_loss = running_loss / len(dataloader)  # Loss'ы делим на кол-во бачей в эпохе \n",
    "            epoch_acc = running_acc / len(dataloader) #считаем Loss на кол-во бачей в эпохе\n",
    " \n",
    "            print(\"{} Loss: {:.4f} Acc: {:.4f} \".format(phase, epoch_loss, epoch_acc), end=\"\")\n",
    "            \n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "            acc_hist[phase].append(epoch_acc)\n",
    "        \n",
    "    return model, loss_hist, acc_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определяем параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Формат pretrained=True - нам нужны веса, которые получились вследствие обучения ResNet на датасете ImageNet\n",
    "model = models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замораживаем веса, чтобы не использовать лишние веса в обучении, а обучать только последний слой\n",
    "#Проходим по параметрам модели (каждый параметр - это каждый слой, model.parameters нам отдаст некоторый итератор по слоям)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "#Для каждого параметра и слоя:\"requires grad = False\", то есть уже не требуется вычисление градиента для данного слоя.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Меняем последний полносвязанный слой, в ResNet он классифицирует на тысячу классов, а у нас класса всего 2.\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "# Cоздаем слой torch.nn.Linear, это полносвязный слой, на вход он принимает model fc_in features. И он единсвенный - разморожен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Затем перекладываем модель на GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Определим loss-функцию и метод оптимизации\n",
    "loss = torch.nn.CrossEntropyLoss()#бинарная кросс-энтропия (у нас всего 2 класса)\n",
    "optimizer = torch.optim.Adam(model.parameters(), amsgrad=True, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Определим планировщик, который будет изменять шаг градиентного спуска\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)#Уменьшаем шаг градиентного спуска каждые 7 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем обучение\n",
    "model, loss, acc = train_model(model, loss, optimizer, scheduler, num_epochs=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строим график Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "for experiment_id in acc.keys():\n",
    "    plt.plot(acc[experiment_id], label=experiment_id)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch num', fontsize=15)\n",
    "plt.ylabel('Accuracy value', fontsize=15);\n",
    "plt.grid(linestyle='--', linewidth=0.5, color='.7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строим график loss функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "for experiment_id in loss.keys():\n",
    "    plt.plot(loss[experiment_id], label=experiment_id)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch num', fontsize=15)\n",
    "plt.ylabel('Loss function value', fontsize=15)\n",
    "plt.grid(linestyle='--', linewidth=0.5, color='.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImageFolder не может обработать путь к папке в которой уже сразу лежат изображения\n",
    "#Копируем всю папку test в директорию test\\unknown\n",
    "test_dir = 'test'\n",
    "shutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Мы не знаем, какие ID, какие названия изображения у нас генерируется, когда мы просим у DataLoader - \"дай нам следующий батч\".\n",
    "#Они по алфавиту идут, по дате создания, или просто случайным образом - непонятно.\n",
    "#Поэтому нам нужно переписать немножко ImageFolder, чтобы он нам отдавал не просто tuple, с самим изображением и его меткой,\n",
    "#а ещё, чтобы он отдавал имя, ну, либо - путь к изображению.\n",
    "\n",
    "#Создаем класс, он наследуется от ImageFolder, но изменяет его функцию get_item\n",
    "class ImageFolderWithPaths(torchvision.datasets.ImageFolder): \n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index) #Дополняем original_tuple путем для файла\n",
    "        #(.__getitem__(index))\n",
    "        path = self.imgs[index][0]\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame\n",
    "\n",
    "#Итерируемся по Crop'ам test датасета\n",
    "for (i,tranforms) in dataset_transforms.items():\n",
    "    test_dataset = ImageFolderWithPaths('/kaggle/working/test', tranforms) #Берем новый класс и получаем tuple из 3х значений\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=0) #Новый даталоадер с путями до изображений\n",
    "     submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})\n",
    "    submission_df['id'] = submission_df['id'].str.replace('/kaggle/working/test/unknown/', '')\n",
    "    submission_df['id'] = submission_df['id'].str.replace('.jpg', '')\n",
    "    submission_df.set_index('id', inplace=True)\n",
    "    \n",
    "    try : df = df.merge(submission_df, how='inner', on='id') #Объединяем в один датафрейм\n",
    "    except BaseException: # Для первой итерации\n",
    "        df = submission_df \n",
    "df.head(8)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() #Переводим модель в состояние eval\n",
    "    test_predictions = []  #Создаем пустой список предсказания \n",
    "    test_img_paths = [] #Пути до изображения\n",
    "    for inputs, labels, paths in tqdm(test_dataloader): #Цикл по test_dataloader inputs - батч с изображением, lable - тут none,\n",
    "        #paths - пути до изображения  \n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)  \n",
    "        with torch.set_grad_enabled(False):\n",
    "            preds = model(inputs) # Считаем предикшены\n",
    "        test_predictions.append(\n",
    "            torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy()) #С помощью torch.nn.functional.softmax получаем\n",
    "        #вероятности, для первого класса [:,1], пеереводим тензор в .data, на .cpu(), в numpy \n",
    "        test_img_paths.extend(paths)\n",
    "    test_predictions = np.concatenate(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels, paths in test_dataloader:\n",
    "    for img, pred in zip(inputs, test_predictions):\n",
    "        show_input(img, title=pred)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получаем среднее по всем crop'ам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean'] = df.mean(axis=1)\n",
    "df.drop(df.columns[:-1], axis='columns', inplace=True)\n",
    "df['label'] = df['mean'].map(lambda pred: 'dirty' if pred > 0.50 else 'cleaned')\n",
    "df.drop(df.columns[:-1], axis='columns', inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохраняем submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf train val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
